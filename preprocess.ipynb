{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Business Dataset<br>\n",
    "Since the records in this dataset do not all correspond to restaurants, we eliminate attributes that do not correspond to \"valid\" businesses <br>\n",
    "Businesses that are not tagged as \"Restaurant\",\"Restaurants\" or \"Food\" are dropped <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pruned_business_file='yelp_academic_dataset_business_pruned.csv'\n",
    "\n",
    "prune=['attributes.HairSpecializesIn.coloring','attributes.HairSpecializesIn.straightperms','attributes.HairSpecializesIn.kids',\n",
    "       'attributes.HairSpecializesIn.extensions','attributes.HairSpecializesIn.curly','attributes.HairSpecializesIn.africanamerican',\n",
    "      'attributes.HairSpecializesIn.asian','attributes.HairSpecializesIn.perms','attributes.AcceptsInsurance','attributes.ByAppointmentOnly'\n",
    "       ,'attributes.RestaurantsAttire','attributes.NoiseLevel','attributes.AgesAllowed','attributes.Alcohol','attributes.BYOBCorkage']\n",
    "\n",
    "header=1\n",
    "for chunk in pd.read_csv('yelp_dataset/csv/yelp_academic_dataset_business.csv',chunksize=1000):\n",
    "    data=chunk.drop(prune,axis=1)\n",
    "    data = data[data['categories'].str.contains(r'(?:\\s|^)(Restaurants|Food)(?:,|$)')==True]\n",
    "    data=data[data['review_count']>=100] #preliminary check for review count\n",
    "    if(header):\n",
    "        data.to_csv(pruned_business_file,mode='a',index=False)\n",
    "        header=0\n",
    "    else:\n",
    "        data.to_csv(pruned_business_file,mode='a',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "For boolean attributes, replace all NaN values with 0.5 <br>\n",
    "Since the data is unavailable we assume a business to both have and not have the corresponding attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(pruned_business_file)\n",
    "data=data.replace(np.nan,0.5)\n",
    "data=data.replace(True,1)\n",
    "data=data.replace(False,0)\n",
    "data=data.replace('True',1)\n",
    "data=data.replace('False',0)\n",
    "data=data.replace('NaN',0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column \"categories\" is a set of all attributes the business is tagged with <br>\n",
    "We create an independent boolean attribute for each category in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_count = dict()\n",
    "\n",
    "for row in data['categories']:\n",
    "    cats = row.split(\", \")\n",
    "    for cat in cats:\n",
    "        if(cat in cat_count.keys()):\n",
    "            cat_count[cat]+=1\n",
    "        else:\n",
    "            cat_count[cat] = 1\n",
    "\n",
    "valid_cats = dict()\n",
    "\n",
    "for key in cat_count.keys():\n",
    "    if(cat_count[key]>50):\n",
    "        valid_cats[key] = cat_count[key]\n",
    "        \n",
    "for key in valid_cats.keys():\n",
    "    data[key] = 0\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    print(i)\n",
    "    cats=data.iloc[i]['categories'].split(\", \")\n",
    "    for cat in cats:\n",
    "        if(cat in valid_cats.keys()):\n",
    "            data[cat].iloc[i]=1\n",
    "\n",
    "data.to_csv(pruned_business_file,mode='w',header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Review Dataset<br>\n",
    "Valid businesses are those remaining in the business file post preprocessing<br>\n",
    "Reviews of invalid businesses are dropped <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_data=pd.read_csv(\"yelp_dataset/yelp_academic_dataset_business_pruned.csv\")\n",
    "valid_businesses=business_data.business_id.unique()\n",
    "\n",
    "pruned_review_file='yelp_academic_dataset_review_pruned.csv'\n",
    "header=1\n",
    "prune=['text']\n",
    "for chunk in pd.read_csv('/Users/malaika/Desktop/yelp_dataset/csv/yelp_academic_dataset_review.csv',chunksize=1000):\n",
    "    data=chunk.drop(prune,axis=1)\n",
    "    data=data.loc[data.business_id.isin(valid_businesses)] \n",
    "    if(header):  \n",
    "        data.to_csv(pruned_review_file,mode='a')\n",
    "        header=0\n",
    "    else:\n",
    "        data.to_csv(pruned_review_file,mode='a',header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the User Dataset<br>\n",
    "Valid users are those having a review count exceeding 100. The cleaned review data is used to extract valid users based on the count of reviews for each user<br>\n",
    "Irrelevant attributes (for our purpose) are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews=pd.read_csv(\"yelp_dataset/yelp_academic_dataset_review_pruned.csv\")\n",
    "count_users=reviews.groupby(by='user_id')\n",
    "groups=count_users.groups.keys()\n",
    "count_users=count_users.count() #find true count of reviews for each user\n",
    "count_users['user_id']=groups \n",
    "valid_users=count_users.loc[count_users.stars>=100]['user_id'] #extract users with review_count>=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pruned_user_file='yelp_academic_dataset_user_pruned.csv'\n",
    "data=pd.read_csv('yelp_dataset/csv/yelp_academic_dataset_user.csv')\n",
    "\n",
    "prune=['elite','useful','compliment_note','compliment_cool','compliment_funny','compliment_hot','compliment_photos',\n",
    "       'compliment_list', 'compliment_writer', 'funny', 'cool','compliment_cute','compliment_profile', 'fans','compliment_plain',\n",
    "       'friends', 'compliment_more']\n",
    "\n",
    "data=data.drop(prune,axis=1)\n",
    "data=data.loc[data.user_id.isin(valid_users)]\n",
    "data.to_csv(pruned_user_file,mode='a',header=True,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
